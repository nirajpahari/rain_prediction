{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "input_df = pd.read_csv(os.path.join(dirname, 'weatherAUS.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RISK_MM</th>\n",
       "      <th>RainTomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-12-01</td>\n",
       "      <td>Albury</td>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>44.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1007.7</td>\n",
       "      <td>1007.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-12-02</td>\n",
       "      <td>Albury</td>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WNW</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1010.6</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-12-03</td>\n",
       "      <td>Albury</td>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WSW</td>\n",
       "      <td>46.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1007.6</td>\n",
       "      <td>1008.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-12-04</td>\n",
       "      <td>Albury</td>\n",
       "      <td>9.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE</td>\n",
       "      <td>24.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1017.6</td>\n",
       "      <td>1012.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.1</td>\n",
       "      <td>26.5</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-12-05</td>\n",
       "      <td>Albury</td>\n",
       "      <td>17.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>41.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1010.8</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>29.7</td>\n",
       "      <td>No</td>\n",
       "      <td>0.2</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
       "0  2008-12-01   Albury     13.4     22.9       0.6          NaN       NaN   \n",
       "1  2008-12-02   Albury      7.4     25.1       0.0          NaN       NaN   \n",
       "2  2008-12-03   Albury     12.9     25.7       0.0          NaN       NaN   \n",
       "3  2008-12-04   Albury      9.2     28.0       0.0          NaN       NaN   \n",
       "4  2008-12-05   Albury     17.5     32.3       1.0          NaN       NaN   \n",
       "\n",
       "  WindGustDir  WindGustSpeed WindDir9am  ... Humidity3pm  Pressure9am  \\\n",
       "0           W           44.0          W  ...        22.0       1007.7   \n",
       "1         WNW           44.0        NNW  ...        25.0       1010.6   \n",
       "2         WSW           46.0          W  ...        30.0       1007.6   \n",
       "3          NE           24.0         SE  ...        16.0       1017.6   \n",
       "4           W           41.0        ENE  ...        33.0       1010.8   \n",
       "\n",
       "   Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  RainToday  RISK_MM  \\\n",
       "0       1007.1       8.0       NaN     16.9     21.8         No      0.0   \n",
       "1       1007.8       NaN       NaN     17.2     24.3         No      0.0   \n",
       "2       1008.7       NaN       2.0     21.0     23.2         No      0.0   \n",
       "3       1012.8       NaN       NaN     18.1     26.5         No      1.0   \n",
       "4       1006.0       7.0       8.0     17.8     29.7         No      0.2   \n",
       "\n",
       "   RainTomorrow  \n",
       "0            No  \n",
       "1            No  \n",
       "2            No  \n",
       "3            No  \n",
       "4            No  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 142193 entries, 0 to 142192\n",
      "Data columns (total 24 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   Date           142193 non-null  object \n",
      " 1   Location       142193 non-null  object \n",
      " 2   MinTemp        141556 non-null  float64\n",
      " 3   MaxTemp        141871 non-null  float64\n",
      " 4   Rainfall       140787 non-null  float64\n",
      " 5   Evaporation    81350 non-null   float64\n",
      " 6   Sunshine       74377 non-null   float64\n",
      " 7   WindGustDir    132863 non-null  object \n",
      " 8   WindGustSpeed  132923 non-null  float64\n",
      " 9   WindDir9am     132180 non-null  object \n",
      " 10  WindDir3pm     138415 non-null  object \n",
      " 11  WindSpeed9am   140845 non-null  float64\n",
      " 12  WindSpeed3pm   139563 non-null  float64\n",
      " 13  Humidity9am    140419 non-null  float64\n",
      " 14  Humidity3pm    138583 non-null  float64\n",
      " 15  Pressure9am    128179 non-null  float64\n",
      " 16  Pressure3pm    128212 non-null  float64\n",
      " 17  Cloud9am       88536 non-null   float64\n",
      " 18  Cloud3pm       85099 non-null   float64\n",
      " 19  Temp9am        141289 non-null  float64\n",
      " 20  Temp3pm        139467 non-null  float64\n",
      " 21  RainToday      140787 non-null  object \n",
      " 22  RISK_MM        142193 non-null  float64\n",
      " 23  RainTomorrow   142193 non-null  object \n",
      "dtypes: float64(17), object(7)\n",
      "memory usage: 26.0+ MB\n"
     ]
    }
   ],
   "source": [
    "input_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making column header lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.columns = map(str.lower, input_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the summary of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using matplotlib to plot a histogram for each numerical attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.hist(bins=50, figsize=(20,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points noted till now:\n",
    "- rainfall, evaporation, windgustspeed, windspped9am, windspeed3pm, riskmm are tail heavy so need to be transformed to have more bell shaped distributions\n",
    "- The attributes have different scales -> feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at all the data that are of type object just to get the understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df[\"location\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df['windgustdir'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df['winddir9am'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df['winddir3pm'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df['raintoday'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df['raintomorrow'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we are going to predict the feature raintomorrow.\n",
    "We can clearly see that this dataset is not balanced.\n",
    "Lets plot the graph too to see the imbalance nature of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.raintomorrow.value_counts(normalize=True).plot(kind='bar', color=['red', 'blue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test split\n",
    "Lets convert our target column raintomorrow and another column raintoday from yes, no to 1, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df['raintoday'] = input_df['raintoday'].map({\"Yes\":1, \"No\": 0})\n",
    "input_df['raintomorrow'] = input_df['raintomorrow'].map({\"Yes\":1, \"No\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets split the data set to train and test set before we handle the imbalance dataset.\n",
    "We will prepare X Y data first.\n",
    "Since we are going to predict whether it is going to rain or not tomorrow, we set raintomorrow as Y and everything else as X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = input_df.drop('raintomorrow', axis=1)\n",
    "# y = input_df['raintomorrow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.head()\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.raintomorrow.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(input_df, input_df['raintomorrow']):\n",
    "    strat_train_set = input_df.loc[train_index]\n",
    "    strat_test_set = input_df.loc[test_index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(strat_train_set.raintomorrow.value_counts())\n",
    "print(strat_train_set.raintomorrow.value_counts()/len(strat_train_set))\n",
    "print(\"-----------------------\")\n",
    "print(strat_test_set.raintomorrow.value_counts()/len(strat_test_set))\n",
    "print(strat_test_set.raintomorrow.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to handle the imbalanced dataset here:\n",
    "According to this [link](https://www.kdnuggets.com/2020/01/5-most-useful-techniques-handle-imbalanced-datasets.html),\n",
    "4 most useful techniques are:\n",
    "- random undersampling and oversampling\n",
    "- undersampling and oversampling using imbalanced learn\n",
    "- class weights in the models\n",
    "- changing teh evaluation metric\n",
    "\n",
    "And according to this [link](https://elitedatascience.com/imbalanced-classes)\n",
    "- upsample minority class\n",
    "- down sample majority class\n",
    "- change the performance metric\n",
    "- penalize algorithms\n",
    "- use tree based algorithms\n",
    "\n",
    "And according to this website [machinelearningmastery](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/), the author provides 8 tactics to combat imbalanced training data\n",
    "1. Collect more data if possible\n",
    "2. Try changing Performance Metric\n",
    "   > Confusion Matrix\n",
    "   > Precision\n",
    "   > Recall\n",
    "   > F1 Score (or F-score)\n",
    "   Further advice to look at:\n",
    "   > Kappa(or Cohen's kappa)\n",
    "   > ROC Curves\n",
    "3. Try resampling the dataset\n",
    "4. Try generate synthetic samples\n",
    "5. Try different Algorithms\n",
    "    > decision trees often perform well on imbalanced datasets\n",
    "6. Try Penalized Models\n",
    "7. Try Different Perspective\n",
    "    > anomaly detection and change detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are multiple methods we use few methods among them and evaluate the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test1: Upsampling minority and down sampling majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_majority = strat_train_set[strat_train_set.raintomorrow==0]\n",
    "df_minority = strat_train_set[strat_train_set.raintomorrow==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minority_upsampled = resample(df_minority, replace=True, n_samples=88252, random_state=42)\n",
    "\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "df_upsampled.raintomorrow.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_majority_downsampled = resample(df_majority, replace=False, n_samples=25502, random_state=42)\n",
    "\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "df_downsampled.raintomorrow.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_up = df_upsampled.corr()\n",
    "corr_matrix_down = df_downsampled.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_up['raintomorrow'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_down['raintomorrow'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data for ML Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will create X and y data for input features and output feature respectively for each sampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_X = df_upsampled.drop(\"raintomorrow\", axis=1)\n",
    "up_y = df_upsampled[\"raintomorrow\"].copy()\n",
    "\n",
    "test_X = strat_test_set.drop(\"raintomorrow\", axis=1)\n",
    "test_y = strat_test_set[\"raintomorrow\"].copy()\n",
    "\n",
    "down_X = df_downsampled.drop(\"raintomorrow\", axis=1)\n",
    "down_y = df_downsampled[\"raintomorrow\"].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "we will use SimpleImputer from scikit-learn to replace each attribute's missing values with the median of that attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer_up = SimpleImputer(strategy=\"median\")\n",
    "imputer_down = SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since median can only be computed on numerical attributes, we need to create a copy of data without object type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_X_num = up_X.drop([\"date\", \"location\", \"windgustdir\", \"winddir9am\", \"winddir3pm\", \"raintoday\"], axis=1)\n",
    "down_X_num = down_X.drop([\"date\", \"location\", \"windgustdir\", \"winddir9am\", \"winddir3pm\", \"raintoday\"], axis=1)\n",
    "\n",
    "up_X_cat = up_X[[\"date\", \"location\", \"windgustdir\", \"winddir9am\", \"winddir3pm\", \"raintoday\"]].copy()\n",
    "down_X_cat = down_X[[\"date\", \"location\", \"windgustdir\", \"winddir9am\", \"winddir3pm\", \"raintoday\"]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will fit the imputer instance to both the upsampled and downsampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_up.fit(up_X_num)\n",
    "imputer_down.fit(down_X_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(up_X_num.median().values)\n",
    "print(\"---------------------\")\n",
    "print(down_X_num.median().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the trained imputer to transform the training set by replacing missing values with the learned medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num_temp_up = imputer_up.transform(up_X_num)\n",
    "X_num_temp_down = imputer_down.transform(down_X_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the results are plain Numpy array, we will create DF for the transformed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_X_num_tr = pd.DataFrame(X_num_temp_up, columns=up_X_num.columns, index=up_X_num.index)\n",
    "down_X_num_tr = pd.DataFrame(X_num_temp_down, columns=down_X_num.columns, index=down_X_num.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_X_num_tr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "According to the book (Hands on machine learning with scikit-learn, keras and tensorflow) there are two common ways to get all attribues to have the same scale: \n",
    "i) min-max scaling\n",
    "ii) standardization\n",
    "\n",
    "since min max scaling is more affected by outliers we will use standardization here.\n",
    "We will use StandardScaler provided by scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_scal_transformer = StandardScaler()\n",
    "\n",
    "temp1 = feat_scal_transformer.fit_transform(up_X_num_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing values in categorical attributes\n",
    "\n",
    "According to this [Link](https://medium.com/analytics-vidhya/ways-to-handle-categorical-column-missing-data-its-implementations-15dc4a56893), there are 3 ways to handle categorical data:\n",
    "1. Frequent Categorical Imputation\n",
    "> Here NaN values are replaced with the most frequent occured category in the column.\n",
    "2. Adding a variable to capture NaN\n",
    "> Here NaN is again replaced by most occuring value and a new feature added to introduce some weight to non imputed and imputed observations\n",
    "3. Create a New Category for NaN Values\n",
    "> here a new category is created for NaN values and added to all NaN values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the amount of NaN values to decide which method to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_X_cat.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(up_X_cat.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 176504 total data and a column with highest NaN value is 12278 ie around 6% so we use the third method ie we will add unknown as the value for all NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "add_unknown_imputer = SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = add_unknown_imputer.fit_transform(up_X_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming date to month only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_only_trans = DateToMonthOnlyTransformer()\n",
    "temp_dtm_trans = dtm_only_trans.fit_transform(up_X_cat.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DateToMonthOnlyTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def get_month( self, obj ):\n",
    "        return str(obj)[5:7]\n",
    "    \n",
    "    def transform(self, X,y=None):\n",
    "        X = X.apply(self.get_month)\n",
    "#         print(X)\n",
    "        return X.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_nan_create_category(DataFrame, ColName):\n",
    "    DataFrame[ColName] = np.where(DataFrame[ColName].isnull(), \"Unknown\", DataFrame[ColName])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = up_X_cat.columns\n",
    "for column in list(up_X_cat.columns):\n",
    "    impute_nan_create_category(up_X_cat, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_X_cat.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling text and categorical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_X_cat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets count the no. of categories in each columns except date and raintoday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Location: \", len(up_X_cat.location.unique()))\n",
    "print(\"WindGustDir: \", len(up_X_cat.windgustdir.unique()))\n",
    "print(\"WindDir9am: \", len(up_X_cat.winddir9am.unique()))\n",
    "print(\"WindDir3pm: \", len(up_X_cat.winddir3pm.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets convert the categorical data using OneHotEncoderTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot_encoder = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add one hot encoding process in the pipeline for only location and other direction categories too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to get only the month from the date. So for this we will create an custom transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateToMonthOnlyTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def get_month( self, obj ):\n",
    "        return str(obj)[5:7]\n",
    "    \n",
    "    def transform(self, X,y=None):\n",
    "        print(X)\n",
    "        print(\"-------------------\")\n",
    "        X = X.apply(self.get_month)\n",
    "        print(X)\n",
    "        return np.array(X.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_converter = DateToMonthOnlyTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(date_converter.transform(up_X_cat.date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "We are going to create the pipeline for the data cleaning step.\n",
    "We will have seperate transformer for numerical data where:\n",
    "* We use simple imputer with mean strategy for filling the NaN values\n",
    "\n",
    "Another transformer will be for the categorical data where:\n",
    "* We will use Simple imputer with strategy constant and fill Unknown for NaN values.\n",
    "* We use OneHotEncoder for the columns with direction data.\n",
    "* We use OrdinalEncoder for the location since the number of locations are too high and one hot encoder will have the vector of high dimension.\n",
    "* We also use the custom transformer we created to change the date into only month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_X_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attributes = ['mintemp', 'maxtemp', 'rainfall', 'evaporation', 'sunshine',\n",
    "       'windgustspeed', 'windspeed9am', 'windspeed3pm', 'humidity9am',\n",
    "       'humidity3pm', 'pressure9am', 'pressure3pm', 'cloud9am', 'cloud3pm',\n",
    "       'temp9am', 'temp3pm', 'risk_mm']\n",
    "cat_attributes = ['windgustdir', 'winddir9am', 'winddir3pm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_trans = Pipeline([\n",
    "    ('simple_imputer', SimpleImputer(strategy=\"mean\")),\n",
    "    ('standard_scaler', StandardScaler())],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical_trans = ColumnTransformer([\n",
    "#     ('simple_imputer', SimpleImputer(strategy=\"mean\"), num_attributes),\n",
    "#     ('standard_scaler', StandardScaler(), num_attributes)],\n",
    "#     remainder=\"passthrough\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "categorical_trans = Pipeline(\n",
    "    [('impute_unknown_for_nan', SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")),\n",
    "#     ('one_hot_dir', OneHotEncoder()),\n",
    "     ('ordinal_cat', OrdinalEncoder())\n",
    "    ]\n",
    ")\n",
    "cat_loc_trans = Pipeline([\n",
    "    ('ordinal_loc', OrdinalEncoder()),]\n",
    ")\n",
    "\n",
    "cat_date_trans = ColumnTransformer([\n",
    "    ('date_transformer', DateToMonthOnlyTransformer(), ['date'])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "# categorical_trans = ColumnTransformer(\n",
    "#     [('impute_unknown_for_nan', SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\"), cat_attributes),\n",
    "# #     ('one_hot_dir', OneHotEncoder(), cat_attributes),\n",
    "#      ('ordinal_cat', OrdinalEncoder(), cat_attributes)\n",
    "#     ],\n",
    "#     remainder=\"passthrough\"\n",
    "# )\n",
    "# cat_loc_trans = ColumnTransformer([\n",
    "#     ('ordinal_loc', OrdinalEncoder(), ['location']),],\n",
    "#     remainder=\"passthrough\"\n",
    "# )\n",
    "\n",
    "# cat_date_trans = ColumnTransformer([\n",
    "#     ('date_transformer', DateToMonthOnlyTransformer(), ['date'])],\n",
    "#     remainder=\"passthrough\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", numerical_trans, num_attributes),\n",
    "    (\"cat\", categorical_trans, cat_attributes),\n",
    "    (\"loc\", cat_loc_trans, ['location']),\n",
    "#     (\"date\", DateToMonthOnlyTransformer(), 'date')\n",
    "], remainder='drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate on the Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [this](https://drgabrielharris.medium.com/python-how-scikit-learn-0-20-optimal-pipeline-and-best-practices-dc4dd94d2c09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "pipeline_tree = Pipeline(steps=[('full', full_pipeline),\n",
    "                           ('classifier', tree)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'classifier__criterion': ['entropy', 'gini'],\n",
    "          'classifier__max_depth': [5,6,7],\n",
    "          'classifier__min_samples_leaf': [4,5,6]}\n",
    "\n",
    "classifier_gs = GridSearchCV(pipeline, params, scoring='roc_auc', cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_gs.fit(up_X, up_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = classifier_gs.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_y, ypred))\n",
    "print(confusion_matrix(test_y, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now use [this](https://www.kdnuggets.com/2018/01/managing-machine-learning-workflows-scikit-learn-pipelines-part-3.html) to extend for many other classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [this site](https://www.kdnuggets.com/2018/01/managing-machine-learning-workflows-scikit-learn-pipelines-part-3.html) for training using multiple models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "pipeline_tree = Pipeline(steps=[('full', full_pipeline),\n",
    "                           ('classifier', tree)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=42)\n",
    "\n",
    "pipeline_lr = Pipeline(steps=[('full', full_pipeline),\n",
    "                              ('classifier', lr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "pipeline_rf = Pipeline(steps=[('full', full_pipeline),\n",
    "                              ('classifier', rf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = svm.SVC(random_state=42)\n",
    "\n",
    "pipeline_svm = Pipeline(steps=[('full', full_pipeline),\n",
    "                              ('classifier', svm)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set grid search params\n",
    "\n",
    "param_range = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "grid_params_dec_tree = [{'classifier__criterion': ['entropy', 'gini'],\n",
    "          'classifier__max_depth': [5,6,7],\n",
    "          'classifier__min_samples_leaf': [4,5,6]}]\n",
    "\n",
    "grid_params_lr = [{'classifier__penalty': ['l1', 'l2'],\n",
    "        'classifier__solver': ['liblinear']}] \n",
    "\n",
    "grid_params_rf = [{'clf__criterion': ['gini', 'entropy'],\n",
    "        'clf__min_samples_leaf': param_range,\n",
    "        'clf__max_depth': param_range,\n",
    "        'clf__min_samples_split': param_range[1:]}]\n",
    "\n",
    "grid_params_svm = [{'clf__kernel': ['linear', 'rbf'], \n",
    "        'clf__C': param_range}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the grid search\n",
    "jobs = -1\n",
    "\n",
    "gs_tree = GridSearchCV(estimator=pipeline_tree,\n",
    "            param_grid=grid_params_dec_tree,\n",
    "            scoring='roc_auc',\n",
    "            cv=5)\n",
    "\n",
    "gs_lr = GridSearchCV(estimator=pipeline_lr,\n",
    "            param_grid=grid_params_lr,\n",
    "            scoring='roc_auc',\n",
    "            cv=10) \n",
    "\n",
    "gs_rf = GridSearchCV(estimator=pipeline_rf,\n",
    "            param_grid=grid_params_rf,\n",
    "            scoring='roc_auc',\n",
    "            cv=10, \n",
    "            n_jobs=jobs)\n",
    "\n",
    "\n",
    "gs_svm = GridSearchCV(estimator=pipeline_svm,\n",
    "            param_grid=grid_params_svm,\n",
    "            scoring='roc_auc',\n",
    "            cv=10,\n",
    "            n_jobs=jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of pipelines for ease of iteration\n",
    "grids = [gs_tree, gs_lr, gs_rf, gs_svm]\n",
    "\n",
    "# Dictionary of pipelines and classifier types for ease of reference\n",
    "grid_dict = {0: 'Decision Tree', 1: 'Logistic Regression', 2: 'Random Forest',\n",
    "             3: 'Support Vector Machine'}\n",
    "# Fit the grid search objects\n",
    "print('Performing model optimizations...')\n",
    "best_acc = 0.0\n",
    "best_clf = 0\n",
    "best_gs = ''\n",
    "for idx, gs in enumerate(grids):\n",
    "    print('\\nEstimator: %s' % grid_dict[idx])\t\n",
    "    # Fit grid search\t\n",
    "    gs.fit(up_X, up_y)\n",
    "    # Best params\n",
    "    print('Best params: %s' % gs.best_params_)\n",
    "    # Best training data accuracy\n",
    "    print('Best training accuracy: %.3f' % gs.best_score_)\n",
    "    # Predict on test data with best params\n",
    "    y_pred = gs.predict(test_X)\n",
    "    # Test data accuracy of model with best params\n",
    "    print('Test set accuracy score for best params: %.3f ' % accuracy_score(test_y, y_pred))\n",
    "    # Track best (highest test accuracy) model\n",
    "    if accuracy_score(test_y, y_pred) > best_acc:\n",
    "        best_acc = accuracy_score(test_y, y_pred)\n",
    "        best_gs = gs\n",
    "        best_clf = idx\n",
    "print('\\nClassifier with best test set accuracy: %s' % grid_dict[best_clf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
